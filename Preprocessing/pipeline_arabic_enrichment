import os
import json
import boto3
import time
import datetime
from io import BytesIO
from dotenv import load_dotenv
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from openai import OpenAI

# ===============================
# LOAD ENVIRONMENT VARIABLES
# ===============================
load_dotenv()
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")

if not GITHUB_TOKEN:
    raise RuntimeError("âŒ ERROR: You must set GITHUB_TOKEN in .env")

client = OpenAI(
    base_url="https://models.github.ai/inference",
    api_key=GITHUB_TOKEN,
)

MODEL_NAME = "openai/gpt-4o-mini"

# ===============================
# S3 CONFIG
# ===============================
BUCKET = "sentiment-data-lake"
RAW_PREFIX = "raw/arabic/"
OUT_PREFIX = "processed/arabic/"

s3 = boto3.client("s3")

# ===============================
# CLEANING
# ===============================
NOISE_PHRASES = [
    "ØªÙ… Ø§Ù„ØªØµÙ…ÙŠÙ… ÙˆØ§Ù„ØªØ·ÙˆÙŠØ± Ø¨ÙˆØ§Ø³Ø·Ø©",
    "Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ¹Ù„Ù‚Ø©",
    "Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…ØªØ¹Ù„Ù‚Ø©",
    "Ø§Ù‚Ø±Ø£ Ø£ÙŠØ¶Ø§",
]

def clean_text(t):
    if not t:
        return ""
    for p in NOISE_PHRASES:
        t = t.replace(p, "")
    return t.strip()

# ===============================
# LOAD ALL JSON FILES FROM S3
# ===============================
def load_raw_articles_from_s3():
    articles = []

    response = s3.list_objects_v2(Bucket=BUCKET, Prefix=RAW_PREFIX)

    if "Contents" not in response:
        return []

    for obj in response["Contents"]:
        key = obj["Key"]

        # skip directory keys
        if key.endswith("/"):
            continue

        # skip useless seen_links.json
        if "seen_links" in key.lower():
            continue

        if not key.endswith(".json"):
            continue

        print(f"ğŸ“¥ Downloading: {key}")

        raw = s3.get_object(Bucket=BUCKET, Key=key)
        content = raw["Body"].read()
        try:
            data = json.loads(content)
        except:
            continue

        if isinstance(data, list):
            for a in data:
                if "full_text" in a and "title" in a:
                    a["full_text"] = clean_text(a["full_text"])
                    articles.append(a)

    return articles

# ===============================
# DEDUPLICATION (FULL-TEXT SIMILARITY)
# ===============================
def deduplicate_articles(articles, threshold=0.85):
    if len(articles) <= 1:
        return articles

    texts = [a["full_text"] for a in articles]
    vectorizer = TfidfVectorizer().fit_transform(texts)
    similarity = cosine_similarity(vectorizer)

    keep = []
    removed = set()

    for i in range(len(articles)):
        if i in removed:
            continue

        keep.append(articles[i])

        for j in range(i + 1, len(articles)):
            if similarity[i][j] >= threshold:
                removed.add(j)

    return keep

# ===============================
# GPT ENRICHMENT
# ===============================
def enrich_article(article):
    full_text = article.get("full_text", "").strip()

    if not full_text:
        return "", "Neutral"

    # --- PRIMARY SAFE PROMPT ---
    prompt_strict = f"""
You will receive Arabic news text that may contain sensitive or violent events.
Your job is to rewrite the information in a SAFE, NON-VIOLENT, HIGH-LEVEL way.

Your output MUST have exactly:
1) SUMMARY_AR: A very short, neutral, non-graphic Arabic summary.
2) SENTIMENT_EN: Positive, Negative, or Neutral.

STRICT RULES:
- DO NOT mention or repeat any violent acts explicitly.
- DO NOT describe crimes, harm, injuries, or conflict details.
- Convert all harmful content into abstract, general wording.
- Keep summary extremely general (e.g., "ØªÙ†Ø§ÙˆÙ„Øª Ø§Ù„Ø£Ø®Ø¨Ø§Ø± ØªØ·ÙˆØ±Ø§Øª ÙÙŠ Ù‚Ø¶ÙŠØ©" instead of details).
- Do NOT add commentary or opinion.
- Do NOT add extra fields.

Text:
----------------
{full_text}
----------------

OUTPUT FORMAT (MANDATORY):
SUMMARY_AR: <summary>
SENTIMENT_EN: <Positive|Negative|Neutral>
"""

    # --- FALLBACK PROMPT (EXTRA SAFE) ---
    prompt_ultrasafe = f"""
You will receive Arabic text. It may reference harmful or sensitive events.
Create:
1) SUMMARY_AR: a very short, extremely safe, general abstracted summary in Arabic.
2) SENTIMENT_EN: Positive, Negative, or Neutral.

DO NOT mention violence, harm, or crimes.
Replace all sensitive content with general wording like:
"Ø§Ù„Ù‚Ø¶ÙŠØ©"ØŒ "Ø§Ù„Ø£Ø­Ø¯Ø§Ø«"ØŒ "Ø§Ù„ØªØ·ÙˆØ±Ø§Øª"ØŒ "Ø§Ù„Ù…Ù„Ù"ØŒ "Ø§Ù„Ø´Ø£Ù† Ø§Ù„Ø¹Ø§Ù…".

Text:
----------------
{full_text}
----------------

OUTPUT FORMAT:
SUMMARY_AR: <summary>
SENTIMENT_EN: <Positive|Negative|Neutral>
"""

    # ========== TRY PRIMARY SAFE PROMPT ==========
    try:
        res = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt_strict}],
            max_tokens=350,
            temperature=0.2,
        )
        response_text = res.choices[0].message.content.strip()

    except Exception as e:
        print(f"[SAFE FALLBACK] Primary prompt blocked, retrying: {e}")

        # ========== FALLBACK PROMPT ==========
        try:
            res = client.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": prompt_ultrasafe}],
                max_tokens=250,
                temperature=0.1,
            )
            response_text = res.choices[0].message.content.strip()

        except Exception as e2:
            print(f"[ERROR] Fallback also failed: {e2}")
            return "", "Neutral"

    # ========== PARSE OUTPUT ==========
    summary = ""
    sentiment = "Neutral"

    if "SUMMARY_AR:" in response_text:
        summary = response_text.split("SUMMARY_AR:")[1].split("SENTIMENT_EN:")[0].strip()

    if "SENTIMENT_EN:" in response_text:
        sentiment = response_text.split("SENTIMENT_EN:")[1].strip()

    if sentiment not in ["Positive", "Negative", "Neutral"]:
        sentiment = "Neutral"

    return summary, sentiment


# ===============================
# SAVE RESULT TO S3
# ===============================
def save_to_s3(articles):
    ts = datetime.datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    key = f"{OUT_PREFIX}processed_arabic_{ts}.json"

    data = json.dumps(articles, ensure_ascii=False, indent=2)
    s3.put_object(
        Bucket=BUCKET,
        Key=key,
        Body=data.encode("utf-8"),
        ContentType="application/json"
    )

    print(f"âœ… Uploaded enriched file to: s3://{BUCKET}/{key}")


# ===============================
# MAIN PIPELINE
# ===============================
def main():
    print("ğŸ”„ Loading raw articles from S3...")
    articles = load_raw_articles_from_s3()
    print(f"â¡ Loaded {len(articles)} raw articles")

    print("ğŸ§¹ Deduplicating...")
    articles = deduplicate_articles(articles)
    print(f"â¡ After deduplication: {len(articles)} articles")

    print("ğŸ¤– Enriching articles with GPT...")
    enriched = []
    for a in articles:
        summary, sentiment = enrich_article(a)
        a["summary"] = summary
        a["sentiment"] = sentiment
        enriched.append(a)

        time.sleep(0.8)  # stay inside GitHub Pro rate limits

    print("ğŸ“¤ Saving to S3...")
    save_to_s3(enriched)

    print("ğŸ‰ DONE")


if __name__ == "__main__":
    main()
