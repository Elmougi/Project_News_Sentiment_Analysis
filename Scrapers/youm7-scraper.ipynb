{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2cd54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")   \n",
    "options.add_argument(\"--disable-gpu\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()),\n",
    "                          options=options)\n",
    "\n",
    "youm7_url = \"https://www.youm7.com\"\n",
    "urgent_new = \"https://www.youm7.com/Section/%D8%A3%D8%AE%D8%A8%D8%A7%D8%B1-%D8%B9%D8%A7%D8%AC%D9%84%D8%A9/65/1\"\n",
    "\n",
    "driver.get(urgent_new)\n",
    "time.sleep(5)  \n",
    "\n",
    "for _ in range(3):     \n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "cards = soup.find_all(\"div\", class_=\"col-xs-12 bigOneSec\")\n",
    "\n",
    "news_list = []\n",
    "\n",
    "for idx, card in enumerate(cards, start=1):\n",
    "    try:\n",
    "        title_tag = card.find(\"h3\").find(\"a\")\n",
    "        title = title_tag.get_text(strip=True)\n",
    "        link = title_tag.get(\"href\")\n",
    "        if not link.startswith(\"http\"):\n",
    "            link = youm7_url + link\n",
    "\n",
    "        img_tag = card.find(\"img\")\n",
    "        image_url = img_tag.get(\"src\") if img_tag else \"\"\n",
    "\n",
    "        date_tag = card.find(\"span\", class_=\"newsDate2\")\n",
    "        pub_date = date_tag.get_text(strip=True) if date_tag else \"\"\n",
    "\n",
    "        desc_tag = card.find(\"p\")\n",
    "        summary = desc_tag.get_text(strip=True) if desc_tag else \"\"\n",
    "\n",
    "        driver.get(link)\n",
    "        time.sleep(2)\n",
    "        article_soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "        paragraphs = article_soup.select(\"div.newsStory p\")\n",
    "        full_text = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "        if not full_text:\n",
    "            paragraphs = article_soup.find_all(\"p\")\n",
    "            full_text = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "        news_list.append({\n",
    "            \"Title\": title,\n",
    "            \"Link\": link,\n",
    "            \"Date\": pub_date,\n",
    "            \"Summary\": summary,\n",
    "            \"Full_Article\": full_text,\n",
    "            \"Image\": image_url\n",
    "        })\n",
    "\n",
    "        print(f\"Scraped ({idx}/{len(cards)}): {title}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping card {idx}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(news_list)\n",
    "df.to_csv(\"youm7_breaking_news.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"DONE! Scraped {len(df)} news articles\")\n",
    "print(\"Saved to youm7_breaking_news.csv\")\n",
    "\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
